{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import load_train_index\n",
    "from pathlib import Path\n",
    "from hw_asr.base.base_text_encoder import BaseTextEncoder\n",
    "from string import ascii_lowercase\n",
    "\n",
    "VOCAB_SIZE = 100\n",
    "\n",
    "index_directory = Path('pretrained_model/index/')\n",
    "\n",
    "tokenizer_directory = Path('pretrained_model/tokenizer')\n",
    "tokenizer_directory.mkdir(exist_ok=True)\n",
    "texts_path = tokenizer_directory / 'texts.txt'\n",
    "model_directory = tokenizer_directory / f'sentence_piece_vocab_{VOCAB_SIZE}'\n",
    "\n",
    "datasets = load_train_index(index_directory)\n",
    "\n",
    "sentences = []\n",
    "for dataset in datasets:\n",
    "    for observation in dataset:\n",
    "        sentences.append(BaseTextEncoder.normalize_text(observation['text']))\n",
    "with open(texts_path, 'w') as f:\n",
    "    print(*sentences, sep='\\n', file=f)\n",
    "    print(*([' '.join(ascii_lowercase)] * len(sentences)), sep='\\n', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "model_prefix = f'sentence_piece_vocab_{VOCAB_SIZE}'\n",
    "model_prefix = tokenizer_directory / model_prefix\n",
    "\n",
    "\n",
    "if not model_prefix.with_suffix('.model').exists():\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=texts_path,\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        model_type='bpe'\n",
    "    )\n",
    "sp_model = spm.SentencePieceProcessor(model_file=str(model_prefix) + '.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it had no ornamentation being exceedingly plain in appearance'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it| ha|d| n|o| o|r|n|a|m|en|t|at|i|on| be|ing| e|x|c|e|ed|ing|ly| p|l|a|in| in| a|p|p|e|ar|an|c|e\n"
     ]
    }
   ],
   "source": [
    "encoded = sp_model.Encode('it had no ornamentation being exceedingly plain in appearance')\n",
    "print('|'.join([sp_model.IdToPiece(c).replace('▁', ' ') for c in encoded]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_lowercase\n",
    "\n",
    "for c in ascii_lowercase:\n",
    "    if sp_model.Decode(sp_model.Encode(c)) != c:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <unk>\n",
      "1 <s>\n",
      "2 </s>\n",
      "3 ▁t\n",
      "4 ▁a\n",
      "5 he\n",
      "6 ▁s\n",
      "7 ▁w\n",
      "8 ▁i\n",
      "9 ▁o\n",
      "10 ▁the\n",
      "11 ▁b\n",
      "12 ▁h\n",
      "13 ▁m\n",
      "14 ▁c\n",
      "15 ▁f\n",
      "16 ▁d\n",
      "17 ▁p\n",
      "18 re\n",
      "19 ▁l\n",
      "20 nd\n",
      "21 ▁n\n",
      "22 ▁e\n",
      "23 ▁g\n",
      "24 in\n",
      "25 ▁y\n",
      "26 er\n",
      "27 ▁u\n",
      "28 ou\n",
      "29 ▁r\n",
      "30 at\n",
      "31 ▁k\n",
      "32 ed\n",
      "33 ▁v\n",
      "34 ▁j\n",
      "35 ▁and\n",
      "36 ▁q\n",
      "37 ▁to\n",
      "38 ▁of\n",
      "39 on\n",
      "40 en\n",
      "41 ▁z\n",
      "42 ▁x\n",
      "43 is\n",
      "44 ing\n",
      "45 ▁th\n",
      "46 ▁he\n",
      "47 or\n",
      "48 es\n",
      "49 as\n",
      "50 ll\n",
      "51 it\n",
      "52 ar\n",
      "53 an\n",
      "54 ▁in\n",
      "55 om\n",
      "56 ▁be\n",
      "57 ▁ha\n",
      "58 le\n",
      "59 ot\n",
      "60 ow\n",
      "61 ic\n",
      "62 ut\n",
      "63 ▁wh\n",
      "64 ▁it\n",
      "65 ld\n",
      "66 ▁that\n",
      "67 ly\n",
      "68 ve\n",
      "69 ▁was\n",
      "70 st\n",
      "71 id\n",
      "72 se\n",
      "73 ▁\n",
      "74 e\n",
      "75 t\n",
      "76 a\n",
      "77 o\n",
      "78 n\n",
      "79 i\n",
      "80 h\n",
      "81 s\n",
      "82 r\n",
      "83 d\n",
      "84 l\n",
      "85 u\n",
      "86 m\n",
      "87 c\n",
      "88 w\n",
      "89 f\n",
      "90 g\n",
      "91 y\n",
      "92 p\n",
      "93 b\n",
      "94 v\n",
      "95 k\n",
      "96 x\n",
      "97 j\n",
      "98 q\n",
      "99 z\n"
     ]
    }
   ],
   "source": [
    "for i in range(sp_model.vocab_size()):\n",
    "    print(i, sp_model.IdToPiece(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[46., 50., 77.,  7., 47., 65., 22., 94., 26., 91., 16., 76., 91.]])\n",
      "tensor([[ 8.,  5., 12., 12., 15., 27., 23., 15., 18., 12.,  4., 27.,  5., 22.,\n",
      "          5., 18., 25., 27.,  4.,  1., 25.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world every day'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import reload\n",
    "reload('hw_asr')\n",
    "from hw_asr.text_encoder.ctc_char_bpe_encoder import CTCCharBpeEncoder  # noqa\n",
    "from hw_asr.text_encoder.ctc_char_bpe_encoder import CTCCharTextEncoder  # noqa\n",
    "\n",
    "encoder = CTCCharBpeEncoder(f'pretrained_model/tokenizer/sentence_piece_vocab_{VOCAB_SIZE}')\n",
    "text_encoder = CTCCharTextEncoder()\n",
    "sentence = 'hello world every day'\n",
    "encoded = encoder.encode(sentence)\n",
    "print(encoded)\n",
    "print(text_encoder.encode(sentence))\n",
    "encoder.ctc_decode_enhanced(encoded[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.,  5., 12., 12., 15., 27., 23., 15., 18., 12.,  4., 27.,  5., 22.,\n",
       "          5., 18., 25., 27.,  4.,  1., 25.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "checkpoint = torch.load('pretrained_model/model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mambaforge\\envs\\cuda_env\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hw_asr.model.deep_speech import DeepSpeech2\n",
    "\n",
    "\n",
    "model = DeepSpeech2(n_feats=128, n_class=28)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "model.fc = nn.Linear(1024, VOCAB_SIZE, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "with open('hw_asr\\configs\\deep_speech_2_server_bpe.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "torch.save({\n",
    "    'state_dict': model.state_dict(),\n",
    "    'monitor_best': 0,\n",
    "    'config': config\n",
    "}, 'tmp/bpe_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
