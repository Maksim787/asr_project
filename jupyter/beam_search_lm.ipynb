{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (0.0%) records are longer then 20.0 seconds. Excluding them.\n",
      "13243 (46.4%) records are longer then 200 characters. Excluding them.\n",
      "Filtered 13243(46.4%) records  from dataset\n",
      "61 (2.3%) records are longer then 20.0 seconds. Excluding them.\n",
      "292 (10.8%) records are longer then 200 characters. Excluding them.\n",
      "Filtered 292(10.8%) records  from dataset\n",
      "41 (1.4%) records are longer then 20.0 seconds. Excluding them.\n",
      "201 (7.0%) records are longer then 200 characters. Excluding them.\n",
      "Filtered 201(7.0%) records  from dataset\n"
     ]
    }
   ],
   "source": [
    "from hw_asr.tests.utils import clear_log_folder_after_use\n",
    "from hw_asr.utils.object_loading import get_dataloaders\n",
    "from hw_asr.utils.parse_config import ConfigParser\n",
    "\n",
    "\n",
    "config_parser = ConfigParser.get_debug_configs()\n",
    "sample_rate = config_parser.config[\"preprocessing\"][\"sr\"]\n",
    "with clear_log_folder_after_use(config_parser):\n",
    "    dataloaders, _ = get_dataloaders(config_parser, config_parser.get_text_encoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw_asr.model.deep_speech import DeepSpeech2\n",
    "\n",
    "model = DeepSpeech2(n_feats=128, n_class=28)\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "checkpoint = torch.load(r'saved/models/deep_speech_2/1024_134159/checkpoint-epoch61.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "def move_batch_to_device(batch):\n",
    "    batch = batch.copy()\n",
    "    for tensor_for_gpu in [\"spectrogram\", \"text_encoded\"]:\n",
    "        batch[tensor_for_gpu] = batch[tensor_for_gpu].to(device)\n",
    "    return batch\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "N_BATCHES = 20\n",
    "batches = []\n",
    "for b in dataloaders['val-other']:\n",
    "    batches.append(b)\n",
    "    if len(batches) == N_BATCHES:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:04<00:00,  3.23s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    log_probs = []\n",
    "    log_probs_length = []\n",
    "    text = []\n",
    "    for b in tqdm(batches):\n",
    "        output = model(**move_batch_to_device(b))\n",
    "        output[\"log_probs\"] = torch.log_softmax(output[\"logits\"], dim=-1)\n",
    "        output[\"log_probs_length\"] = model.transform_input_lengths(b[\"spectrogram_length\"])\n",
    "        for i in range(len(b['text'])):\n",
    "            log_probs.append(output['log_probs'][i])\n",
    "            log_probs_length.append(output['log_probs_length'][i])\n",
    "            text.append(b['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXAMPLES = N_BATCHES * dataloaders['val-other'].batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 640/640 [00:28<00:00, 22.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from hw_asr.text_encoder.ctc_char_text_encoder import CTCCharTextEncoder\n",
    "\n",
    "text_encoder = CTCCharTextEncoder()\n",
    "\n",
    "pred_argmax = []\n",
    "for i in range(N_EXAMPLES):\n",
    "    log_prob_vec = torch.argmax(log_probs[i].cpu(), dim=-1).numpy()\n",
    "    pred_text = text_encoder.ctc_decode_enhanced(log_prob_vec[:log_probs_length[i]])\n",
    "    pred_argmax.append(pred_text)\n",
    "\n",
    "pred_beam_search = [text_encoder.ctc_beam_search(log_probs[i], log_probs_length[i], beam_size=5)[0].text for i in tqdm(range(N_EXAMPLES))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.openslr.org/resources/11/3-gram.pruned.1e-7.arpa.gz to lm_models/3-gram.pruned.1e-7.arpa.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3-gram.pruned.1e-7.arpa.gz: 34.1MB [00:08, 3.96MB/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://www.openslr.org/resources/11/librispeech-vocab.txt to lm_models/librispeech-vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "librispeech-vocab.txt: 1.74MB [00:01, 1.15MB/s]                            \n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "from speechbrain.utils.data_utils import download_file\n",
    "\n",
    "LM_MODELS_DIRECTORY = Path('lm_models/')\n",
    "LM_MODELS_DIRECTORY.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_URL = 'https://www.openslr.org/resources/11/3-gram.pruned.1e-7.arpa.gz'\n",
    "VOCAB_URL = 'http://www.openslr.org/resources/11/librispeech-vocab.txt'\n",
    "\n",
    "MODEL_PATH = LM_MODELS_DIRECTORY / '3-gram.pruned.1e-7.arpa'\n",
    "VOCAB_PATH = LM_MODELS_DIRECTORY / 'librispeech-vocab.txt'\n",
    "\n",
    "def download_lm():\n",
    "    if not MODEL_PATH.exists():\n",
    "        extract_path = LM_MODELS_DIRECTORY / '3-gram.pruned.1e-7.arpa.gz'\n",
    "        # Download file\n",
    "        download_file(MODEL_URL, extract_path)\n",
    "        # Extract file\n",
    "        with gzip.open(extract_path, 'rb') as f_in, open(MODEL_PATH, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(str(extract_path))\n",
    "        # Convert to lowercase\n",
    "        with open(MODEL_PATH) as f:\n",
    "            content = f.read()\n",
    "        with open(MODEL_PATH, 'w') as f:\n",
    "            f.write(content.lower().replace(\"\\'\", '').replace(\"\\\"\", ''))\n",
    "    download_file(VOCAB_URL, VOCAB_PATH)\n",
    "\n",
    "\n",
    "download_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using arpa instead of binary LM file, decoder instantiation might be slow.\n",
      "Alphabet determined to be of regular style.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /home/ubuntu/asr_project/lm_models/3-gram.pruned.1e-7.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "\n",
    "with open(VOCAB_PATH) as f:\n",
    "    unigram_list = [t.lower() for t in f.read().strip().split(\"\\n\")]\n",
    "\n",
    "\n",
    "decoder = build_ctcdecoder(\n",
    "    [''] + text_encoder.alphabet,\n",
    "    str(MODEL_PATH),\n",
    "    unigram_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import reload\n",
    "\n",
    "reload('hw_asr')\n",
    "\n",
    "from hw_asr.text_encoder.ctc_char_text_encoder import CTCCharTextEncoder\n",
    "\n",
    "encoder = CTCCharTextEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "encoder.load_lm()\n",
    "with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n",
    "    pred_lm = encoder.ctc_beam_search_lm(log_probs, log_probs_length, beam_size=2000, pool=pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "def autocorrect_sentence(sentence):\n",
    "    blob = TextBlob(sentence)\n",
    "    corrected_sentence = blob.correct()\n",
    "    return str(corrected_sentence)\n",
    "\n",
    "pred_corr = [autocorrect_sentence(s) for s in pred_lm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArgMax\n",
      "examples = 640\n",
      "WER = 32.091\tCER = 12.14\n",
      "BeamSearch\n",
      "examples = 640\n",
      "WER = 31.610\tCER = 11.96\n",
      "BeamSearch + LM\n",
      "examples = 640\n",
      "WER = 21.997\tCER = 9.76\n",
      "BeamSearch + LM + correction\n",
      "examples = 640\n",
      "WER = 22.722\tCER = 10.39\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from hw_asr.metric.utils import calc_cer, calc_wer\n",
    "\n",
    "\n",
    "def print_wer_cer(targets, predictions):\n",
    "    assert len(targets) == len(predictions)\n",
    "    print(f'examples = {len(targets)}')\n",
    "    wer = np.mean([calc_wer(target, prediction) for target, prediction in zip(targets, predictions)])\n",
    "    cer = np.mean([calc_cer(target, prediction) for target, prediction in zip(targets, predictions)])\n",
    "    print(f'WER = {wer * 100:.3f}\\tCER = {cer * 100:.2f}')\n",
    "\n",
    "\n",
    "print('ArgMax')\n",
    "print_wer_cer(text, pred_argmax)\n",
    "print('BeamSearch')\n",
    "print_wer_cer(text, pred_beam_search)\n",
    "print('BeamSearch + LM')\n",
    "print_wer_cer(text, pred_lm)\n",
    "print('BeamSearch + LM + correction')\n",
    "print_wer_cer(text, pred_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "10/10\n",
      "ArgMax\n",
      "examples = 10\n",
      "WER = 33.289\tCER = 12.69\n",
      "BeamSearch\n",
      "examples = 10\n",
      "WER = 31.825\tCER = 11.85\n",
      "BeamSearch + LM\n",
      "examples = 10\n",
      "WER = 32.225\tCER = 12.54\n"
     ]
    }
   ],
   "source": [
    "N = N_EXAMPLES\n",
    "N = 10\n",
    "pred_lm = []\n",
    "\n",
    "text_encoder.alpha_len = 2.35\n",
    "text_encoder.alpha_lm = 0.5\n",
    "\n",
    "text_encoder.use_lm = True\n",
    "for i in range(N):\n",
    "    print(f'{i + 1}/{N}')\n",
    "    pred_lm.append(text_encoder.ctc_beam_search(log_probs[i], log_probs_length[i], beam_size=30)[0].text)\n",
    "\n",
    "print('ArgMax')\n",
    "print_wer_cer(text[:N], pred_argmax[:N])\n",
    "print('BeamSearch')\n",
    "print_wer_cer(text[:N], pred_beam_search[:N])\n",
    "print('BeamSearch + LM')\n",
    "print_wer_cer(text[:N], pred_lm[:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:27<00:00,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples = 100\n",
      "WER = 34.067\tCER = 13.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_wer_cer(text[:N], [text_encoder.ctc_beam_search(log_probs[i], log_probs_length[i], beam_size=30)[0].text for i in tqdm(range(N))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.37425800000000226"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.log_s('i', eos=False) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but now the brandon was a ful swing'"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_beam_search[N - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but now the brandon was a fuol swing'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_lm[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
