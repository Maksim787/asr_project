{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (0.0%) records are longer then 20.0 seconds. Excluding them.\n",
      "13243 (46.4%) records are longer then 200 characters. Excluding them.\n",
      "Filtered 13243(46.4%) records  from dataset\n",
      "61 (2.3%) records are longer then 20.0 seconds. Excluding them.\n",
      "292 (10.8%) records are longer then 200 characters. Excluding them.\n",
      "Filtered 292(10.8%) records  from dataset\n",
      "41 (1.4%) records are longer then 20.0 seconds. Excluding them.\n",
      "201 (7.0%) records are longer then 200 characters. Excluding them.\n",
      "Filtered 201(7.0%) records  from dataset\n"
     ]
    }
   ],
   "source": [
    "from hw_asr.tests.utils import clear_log_folder_after_use\n",
    "from hw_asr.utils.object_loading import get_dataloaders\n",
    "from hw_asr.utils.parse_config import ConfigParser\n",
    "\n",
    "\n",
    "config_parser = ConfigParser.get_debug_configs()\n",
    "sample_rate = config_parser.config[\"preprocessing\"][\"sr\"]\n",
    "with clear_log_folder_after_use(config_parser):\n",
    "    dataloaders = get_dataloaders(config_parser, config_parser.get_text_encoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw_asr.model.deep_speech import DeepSpeech2\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "model = DeepSpeech2(n_feats=128, n_class=28)\n",
    "checkpoint = torch.load(r'saved_server\\models\\deep_speech_2\\1024_134159\\checkpoint-epoch61.pth')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "def move_batch_to_device(batch):\n",
    "    batch = batch.copy()\n",
    "    for tensor_for_gpu in [\"spectrogram\", \"text_encoded\"]:\n",
    "        batch[tensor_for_gpu] = batch[tensor_for_gpu].to(device)\n",
    "    return batch\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "N_BATCHES = 20\n",
    "batches = []\n",
    "for b in dataloaders['val-other']:\n",
    "    batches.append(b)\n",
    "    if len(batches) == N_BATCHES:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:18<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    log_probs = []\n",
    "    log_probs_length = []\n",
    "    logits = []\n",
    "    text = []\n",
    "    for b in tqdm(batches):\n",
    "        output = model(**move_batch_to_device(b))\n",
    "        output[\"log_probs\"] = torch.log_softmax(output[\"logits\"], dim=-1)\n",
    "        output[\"log_probs_length\"] = model.transform_input_lengths(b[\"spectrogram_length\"])\n",
    "        for i in range(len(b['text'])):\n",
    "            log_probs.append(output['log_probs'][i])\n",
    "            log_probs_length.append(output['log_probs_length'][i])\n",
    "            text.append(b['text'][i])\n",
    "            logits.append(output['logits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXAMPLES = N_BATCHES * dataloaders['val-other'].batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 640/640 [01:02<00:00, 10.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from hw_asr.text_encoder.ctc_char_text_encoder import CTCCharTextEncoder\n",
    "\n",
    "text_encoder = CTCCharTextEncoder()\n",
    "\n",
    "pred_argmax = []\n",
    "for i in range(N_EXAMPLES):\n",
    "    log_prob_vec = torch.argmax(log_probs[i].cpu(), dim=-1).numpy()\n",
    "    pred_text = text_encoder.ctc_decode_enhanced(log_prob_vec[:log_probs_length[i]])\n",
    "    pred_argmax.append(pred_text)\n",
    "\n",
    "pred_beam_search = [text_encoder.ctc_beam_search(log_probs[i], log_probs_length[i], beam_size=5)[0].text for i in tqdm(range(N_EXAMPLES))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_models\\librispeech-vocab.txt exists. Skipping download\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "from speechbrain.utils.data_utils import download_file\n",
    "\n",
    "OUT_DIRECTORY = Path('lm_models/')\n",
    "OUT_DIRECTORY.mkdir(exist_ok=True)\n",
    "\n",
    "MODEL_URL = 'https://www.openslr.org/resources/11/3-gram.pruned.1e-7.arpa.gz'\n",
    "VOCAB_URL = 'http://www.openslr.org/resources/11/librispeech-vocab.txt'\n",
    "\n",
    "MODEL_PATH = OUT_DIRECTORY / '3-gram.pruned.1e-7.arpa'\n",
    "VOCAB_PATH = OUT_DIRECTORY / 'librispeech-vocab.txt'\n",
    "\n",
    "def download_lm():\n",
    "    if not MODEL_PATH.exists():\n",
    "        extract_path = OUT_DIRECTORY / '3-gram.pruned.1e-7.arpa.gz'\n",
    "        # Download file\n",
    "        download_file(MODEL_URL, extract_path)\n",
    "        # Extract file\n",
    "        with gzip.open(extract_path, 'rb') as f_in, open(MODEL_PATH, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(str(extract_path))\n",
    "        # Convert to lowercase\n",
    "        with open(MODEL_PATH) as f:\n",
    "            content = f.read()\n",
    "        with open(MODEL_PATH, 'w') as f:\n",
    "            f.write(content.lower())\n",
    "    download_file(VOCAB_URL, VOCAB_PATH)\n",
    "\n",
    "\n",
    "download_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "import kenlm\n",
    "\n",
    "\n",
    "with open(VOCAB_PATH) as f:\n",
    "    unigram_list = [t.lower() for t in f.read().strip().split(\"\\n\")]\n",
    "\n",
    "# load kenlm Model\n",
    "kenlm_model = kenlm.Model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function build_ctcdecoder in module pyctcdecode.decoder:\n",
      "\n",
      "build_ctcdecoder(labels: 'List[str]', kenlm_model_path: 'Optional[str]' = None, unigrams: 'Optional[Collection[str]]' = None, alpha: 'float' = 0.5, beta: 'float' = 1.5, unk_score_offset: 'float' = -10.0, lm_score_boundary: 'bool' = True) -> 'BeamSearchDecoderCTC'\n",
      "    Build a BeamSearchDecoderCTC instance with main functionality.\n",
      "    \n",
      "    Args:\n",
      "        labels: class containing the labels for input logit matrices\n",
      "        kenlm_model_path: path to kenlm n-gram language model\n",
      "        unigrams: list of known word unigrams\n",
      "        alpha: weight for language model during shallow fusion\n",
      "        beta: weight for length score adjustment of during scoring\n",
      "        unk_score_offset: amount of log score offset for unknown tokens\n",
      "        lm_score_boundary: whether to have kenlm respect boundaries when scoring\n",
      "    \n",
      "    Returns:\n",
      "        instance of BeamSearchDecoderCTC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(build_ctcdecoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = build_ctcdecoder(\n",
    "    asr_model.decoder.vocabulary,\n",
    "    kenlm_model,\n",
    "    unigram_list,\n",
    ")\n",
    "decoder.decode(logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArgMax\n",
      "examples = 640\n",
      "WER = 30.952\tCER = 11.99\n",
      "BeamSearch\n",
      "examples = 640\n",
      "WER = 30.407\tCER = 11.84\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from hw_asr.metric.utils import calc_cer, calc_wer\n",
    "\n",
    "\n",
    "def print_wer_cer(targets, predictions):\n",
    "    assert len(targets) == len(predictions)\n",
    "    print(f'examples = {len(targets)}')\n",
    "    wer = np.mean([calc_wer(target, prediction) for target, prediction in zip(targets, predictions)])\n",
    "    cer = np.mean([calc_cer(target, prediction) for target, prediction in zip(targets, predictions)])\n",
    "    print(f'WER = {wer * 100:.3f}\\tCER = {cer * 100:.2f}')\n",
    "\n",
    "\n",
    "print('ArgMax')\n",
    "print_wer_cer(text, pred_argmax)\n",
    "print('BeamSearch')\n",
    "print_wer_cer(text, pred_beam_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "10/10\n",
      "ArgMax\n",
      "examples = 10\n",
      "WER = 33.289\tCER = 12.69\n",
      "BeamSearch\n",
      "examples = 10\n",
      "WER = 31.825\tCER = 11.85\n",
      "BeamSearch + LM\n",
      "examples = 10\n",
      "WER = 32.225\tCER = 12.54\n"
     ]
    }
   ],
   "source": [
    "N = N_EXAMPLES\n",
    "N = 10\n",
    "pred_lm = []\n",
    "\n",
    "text_encoder.alpha_len = 2.35\n",
    "text_encoder.alpha_lm = 0.5\n",
    "\n",
    "text_encoder.use_lm = True\n",
    "for i in range(N):\n",
    "    print(f'{i + 1}/{N}')\n",
    "    pred_lm.append(text_encoder.ctc_beam_search(log_probs[i], log_probs_length[i], beam_size=30)[0].text)\n",
    "\n",
    "print('ArgMax')\n",
    "print_wer_cer(text[:N], pred_argmax[:N])\n",
    "print('BeamSearch')\n",
    "print_wer_cer(text[:N], pred_beam_search[:N])\n",
    "print('BeamSearch + LM')\n",
    "print_wer_cer(text[:N], pred_lm[:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:27<00:00,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples = 100\n",
      "WER = 34.067\tCER = 13.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_wer_cer(text[:N], [text_encoder.ctc_beam_search(log_probs[i], log_probs_length[i], beam_size=30)[0].text for i in tqdm(range(N))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.37425800000000226"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.log_s('i', eos=False) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but now the brandon was a ful swing'"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_beam_search[N - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but now the brandon was a fuol swing'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_lm[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
